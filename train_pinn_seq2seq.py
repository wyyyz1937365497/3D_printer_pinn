# train_pinn_seq2seq.py (å®Œæ•´æ”¹è¿›ç‰ˆ)
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
import os
import time
import gc
import argparse
from torch.amp import autocast, GradScaler
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR
import pickle
import matplotlib.pyplot as plt
import signal
import atexit

# ==================== é…ç½®å‚æ•° ====================
class Config:
    def __init__(self):
        self.data_path = 'enterprise_dataset/printer_enterprise_data.csv'
        self.seq_len = 200          # å†å²é•¿åº¦
        self.pred_len = 50          # é¢„æµ‹é•¿åº¦
        self.batch_size = 256
        self.gradient_accumulation_steps = 4
        self.model_dim = 256
        self.num_heads = 8
        self.num_layers = 6
        self.dim_feedforward = 1024
        self.dropout = 0.1
        self.lr = 2e-4
        self.epochs = 30
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.num_workers = 4
        self.max_samples = 200000
        self.lambda_physics = 0.05
        self.warmup_epochs = 5
        self.checkpoint_dir = './checkpoints_seq2seq'
        self.resume_from = None
        self.save_on_exit = True
        self.save_interval = 5
        self.start_epoch = 0
        self.load_optimizer_state = True  # ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æ§åˆ¶
        self.original_batch_size = None   # åŸå§‹batch sizeï¼ˆç”¨äºå­¦ä¹ ç‡ç¼©æ”¾ï¼‰
        
        # åˆ—å®šä¹‰
        self.ctrl_cols = ['ctrl_T_target', 'ctrl_speed_set', 'ctrl_heater_base']
        self.state_cols = ['temperature_C', 'vibration_disp_m', 'vibration_vel_m_s',
                          'motor_current_A', 'pressure_bar', 'acoustic_signal']
        
        # ç»´åº¦å®šä¹‰
        self.input_dim = len(self.ctrl_cols) + len(self.state_cols)
        self.output_dim = len(self.state_cols)
        self.ctrl_dim = len(self.ctrl_cols)

# ==================== ä½ç½®ç¼–ç  ====================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

# ==================== Seq2Seq æ¨¡å‹ ====================
class PrinterPINN_Seq2Seq(nn.Module):
    def __init__(self, config):
        super(PrinterPINN_Seq2Seq, self).__init__()
        
        self.input_dim = config.input_dim
        self.output_dim = config.output_dim
        self.ctrl_dim = config.ctrl_dim
        self.d_model = config.model_dim
        self.pred_len = config.pred_len
        
        # Encoder
        self.encoder_embedding = nn.Linear(self.input_dim, self.d_model)
        self.pos_encoder = PositionalEncoding(self.d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=config.num_heads,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)
        
        # Decoder
        self.decoder_embedding = nn.Linear(self.ctrl_dim, self.d_model)
        
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=self.d_model,
            nhead=config.num_heads,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            batch_first=True
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=config.num_layers)
        
        # Output
        self.fc_out = nn.Linear(self.d_model, self.output_dim)

    def forward(self, src, tgt_ctrl):
        # Encoder
        src_emb = self.encoder_embedding(src)
        src_emb = self.pos_encoder(src_emb)
        memory = self.encoder(src_emb)
        
        # Decoder
        tgt_emb = self.decoder_embedding(tgt_ctrl)
        tgt_emb = self.pos_encoder(tgt_emb)
        output = self.decoder(tgt_emb, memory)
        
        prediction = self.fc_out(output)
        return prediction

    def physics_loss(self, y_pred, y_true):
        """ç‰©ç†çº¦æŸæŸå¤±ï¼ˆé’ˆå¯¹åºåˆ—ï¼‰"""
        loss = 0.0
        batch_size, seq_len, _ = y_pred.shape
        
        # çƒ­ä¼ å¯¼æŸå¤±ï¼šæ¸©åº¦å˜åŒ–å¹³æ»‘
        temp_pred = y_pred[:, :, 0]
        dT_pred = torch.diff(temp_pred, dim=1)
        d2T_pred = torch.diff(dT_pred, dim=1)
        loss += torch.mean(d2T_pred ** 2)
        
        # æŒ¯åŠ¨èƒ½é‡å®ˆæ’
        disp_pred = y_pred[:, :, 1]
        vel_pred = y_pred[:, :, 2]
        dt = 1.0
        vel_from_disp = torch.diff(disp_pred, dim=1) / dt
        loss += torch.mean((vel_from_disp - vel_pred[:, :-1]) ** 2)
        
        return loss

# ==================== æ•°æ®å¤„ç†å™¨ ====================
class MemoryDataProcessor:
    def __init__(self, data_path, seq_len, pred_len, max_samples, config):
        self.data_path = data_path
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.max_samples = max_samples
        self.config = config
        
        self.input_dim = len(self.config.ctrl_cols) + len(self.config.state_cols)
        self.output_dim = len(self.config.state_cols)
        self.ctrl_dim = len(self.config.ctrl_cols)
        
        print(f"ğŸ”„ å¼€å§‹å¤„ç†æ•°æ®...")
        print(f"ğŸ“Š å†å²é•¿åº¦: {seq_len}, é¢„æµ‹é•¿åº¦: {pred_len}")
        self.process_data()

    def process_data(self):
        """å¤„ç†æ•°æ®ç”¨äºSeq2Seqè®­ç»ƒ"""
        df = pd.read_csv(self.data_path)
        print(f"âœ… åŸå§‹æ•°æ®åŠ è½½: {df.shape}")
        
        numeric_cols = self.config.ctrl_cols + self.config.state_cols + ['fault_label']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = df[col].astype(np.float32)
        
        all_cols = self.config.ctrl_cols + self.config.state_cols
        
        grouped = df.groupby('machine_id')
        samples = []
        count = 0
        
        print("ğŸ“Š æ”¶é›†æ ·æœ¬ç´¢å¼•...")
        for machine_id, group in grouped:
            group = group.sort_values('timestamp').reset_index(drop=True)
            data_array = group[all_cols].values
            ctrl_array = group[self.config.ctrl_cols].values
            state_array = group[self.config.state_cols].values
            fault_array = group['fault_label'].values if 'fault_label' in group.columns else np.zeros(len(group))
            
            total_len = len(group)
            required_len = self.seq_len + self.pred_len
            
            if total_len < required_len:
                continue
            
            n_windows = total_len - required_len + 1
            
            for i in range(n_windows):
                if count >= self.max_samples:
                    break
                
                window_fault = fault_array[i:i+required_len]
                if np.any(window_fault == 1):
                    continue
                
                x_hist = data_array[i:i+self.seq_len]
                x_future_ctrl = ctrl_array[i+self.seq_len:i+required_len]
                y_future_state = state_array[i+self.seq_len:i+required_len]
                
                samples.append((x_hist, x_future_ctrl, y_future_state))
                count += 1
            
            if count >= self.max_samples:
                break
        
        self.total_samples = len(samples)
        self.split_idx = int(self.total_samples * 0.8)
        
        train_samples = samples[:self.split_idx]
        val_samples = samples[self.split_idx:]
        
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {self.total_samples}")
        print(f"   è®­ç»ƒé›†: {len(train_samples)}, éªŒè¯é›†: {len(val_samples)}")
        
        print("ğŸ“Š è®¡ç®—ç»Ÿè®¡é‡...")
        all_x_hist = np.array([s[0] for s in train_samples])
        all_y_future = np.array([s[2] for s in train_samples])
        
        self.mean_X = all_x_hist.mean(axis=(0, 1))
        self.std_X = all_x_hist.std(axis=(0, 1))
        self.mean_Y = all_y_future.mean(axis=(0, 1))
        self.std_Y = all_y_future.std(axis=(0, 1))
        
        self.std_X[self.std_X < 1e-8] = 1.0
        self.std_Y[self.std_Y < 1e-8] = 1.0
        
        print(f"   Input Mean: {self.mean_X}")
        print(f"   Input Std: {self.std_X}")
        print(f"   Output Mean: {self.mean_Y}")
        print(f"   Output Std: {self.std_Y}")
        
        self.train_X = np.zeros((len(train_samples), self.seq_len, self.input_dim), dtype=np.float32)
        self.train_ctrl = np.zeros((len(train_samples), self.pred_len, self.ctrl_dim), dtype=np.float32)
        self.train_Y = np.zeros((len(train_samples), self.pred_len, self.output_dim), dtype=np.float32)
        
        for idx, (x_hist, x_ctrl, y_state) in enumerate(train_samples):
            self.train_X[idx] = (x_hist - self.mean_X) / self.std_X
            self.train_ctrl[idx] = (x_ctrl - self.mean_X[:self.ctrl_dim]) / self.std_X[:self.ctrl_dim]
            self.train_Y[idx] = (y_state - self.mean_Y) / self.std_Y
        
        self.val_X = np.zeros((len(val_samples), self.seq_len, self.input_dim), dtype=np.float32)
        self.val_ctrl = np.zeros((len(val_samples), self.pred_len, self.ctrl_dim), dtype=np.float32)
        self.val_Y = np.zeros((len(val_samples), self.pred_len, self.output_dim), dtype=np.float32)
        
        for idx, (x_hist, x_ctrl, y_state) in enumerate(val_samples):
            self.val_X[idx] = (x_hist - self.mean_X) / self.std_X
            self.val_ctrl[idx] = (x_ctrl - self.mean_X[:self.ctrl_dim]) / self.std_X[:self.ctrl_dim]
            self.val_Y[idx] = (y_state - self.mean_Y) / self.std_Y
        
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼")

    def inverse_transform_y(self, y_norm):
        return y_norm * self.std_Y + self.mean_Y

# ==================== æ•°æ®é›†ç±» ====================
class Seq2SeqDataset(Dataset):
    def __init__(self, X_hist, X_ctrl, Y):
        self.X_hist = torch.from_numpy(X_hist)
        self.X_ctrl = torch.from_numpy(X_ctrl)
        self.Y = torch.from_numpy(Y)

    def __len__(self):
        return self.X_hist.shape[0]

    def __getitem__(self, idx):
        return self.X_hist[idx], self.X_ctrl[idx], self.Y[idx]

def seq2seq_collate_fn(batch):
    x_hist, x_ctrl, y = zip(*batch)
    import torch
    # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯tensorç±»å‹
    x_hist = torch.stack([torch.from_numpy(x) if isinstance(x, np.ndarray) else x for x in x_hist])
    x_ctrl = torch.stack([torch.from_numpy(x) if isinstance(x, np.ndarray) else x for x in x_ctrl])
    y = torch.stack([torch.from_numpy(y) if isinstance(y, np.ndarray) else y for y in y])
    return x_hist, x_ctrl, y

# ==================== è®­ç»ƒçŠ¶æ€ç®¡ç†å™¨ ====================
class TrainingStateManager:
    """ç®¡ç†è®­ç»ƒçŠ¶æ€ï¼Œç”¨äºä¼˜é›…é€€å‡ºå’Œæ¢å¤"""
    def __init__(self, config, model, optimizer, scheduler, checkpoint_dir):
        self.config = config
        self.model = model
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.checkpoint_dir = checkpoint_dir
        self.current_epoch = 0
        self.train_loss = 0.0
        self.val_loss = 0.0
        self.best_val_loss = float('inf')
        self.training_start_time = time.time()
        self.last_epoch_time = time.time()
        
    def update_epoch(self, epoch, train_loss, val_loss):
        self.current_epoch = epoch
        self.train_loss = train_loss
        self.val_loss = val_loss
        
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            
        self.last_epoch_time = time.time()
    
    def save_checkpoint(self, filename):
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_loss': self.train_loss,
            'val_loss': self.val_loss,
            'best_val_loss': self.best_val_loss,
            'config': self.config.__dict__,
            'training_start_time': self.training_start_time
        }
        torch.save(checkpoint, filename)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")
    
    def get_time_info(self):
        """è·å–æ—¶é—´ä¿¡æ¯å­—å…¸"""
        elapsed_total = time.time() - self.training_start_time
        epochs_completed = self.current_epoch - self.config.start_epoch
        remaining_epochs = self.config.epochs - self.current_epoch
        
        if epochs_completed > 0:
            avg_epoch_time = elapsed_total / epochs_completed
            eta_seconds = avg_epoch_time * remaining_epochs
        else:
            avg_epoch_time = 0
            eta_seconds = 0
            
        return {
            'elapsed_total': elapsed_total,
            'elapsed_formatted': format_time(elapsed_total),
            'avg_epoch_time': avg_epoch_time,
            'eta_seconds': eta_seconds,
            'eta_formatted': format_time(eta_seconds),
            'progress_percent': (self.current_epoch / self.config.epochs) * 100
        }

# ==================== å·¥å…·å‡½æ•° ====================
def format_time(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"

def load_checkpoint(model, optimizer, scheduler, filename, load_optimizer_state=True, verbose=True):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    checkpoint = torch.load(filename, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    
    if load_optimizer_state and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    if load_optimizer_state and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
    start_epoch = checkpoint.get('epoch', 0)
    best_val_loss = checkpoint.get('best_val_loss', float('inf'))
    train_loss = checkpoint.get('train_loss', 0)
    val_loss = checkpoint.get('val_loss', 0)
    
    # è·å–åŸå§‹batch sizeï¼ˆç”¨äºå­¦ä¹ ç‡è°ƒæ•´ï¼‰
    original_config = checkpoint.get('config', {})
    original_batch_size = original_config.get('batch_size', None)
    
    if verbose:
        print(f"âœ… æ£€æŸ¥ç‚¹å·²åŠ è½½: {filename}")
        print(f"   ä»Epoch {start_epoch}å¼€å§‹ç»§ç»­è®­ç»ƒ")
        print(f"   å½“å‰éªŒè¯æŸå¤±: {val_loss:.6f}")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        if original_batch_size:
            print(f"   åŸå§‹Batch Size: {original_batch_size}")
    
    return start_epoch, train_loss, val_loss, best_val_loss, original_batch_size

def save_checkpoint(epoch, model, optimizer, scheduler, train_loss, val_loss, best_val_loss, config, filename):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'best_val_loss': best_val_loss,
        'config': config.__dict__
    }
    torch.save(checkpoint, filename)
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_pinn_seq2seq(config):
    print("=" * 70)
    print("ğŸš€ PrinterPINN Seq2Seq è®­ç»ƒ")
    print("=" * 70)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"   Batch Size: {config.batch_size}")
    print(f"   Gradient Accumulation: {config.gradient_accumulation_steps}")
    print(f"   Effective Batch Size: {config.batch_size * config.gradient_accumulation_steps}")
    print(f"   Learning Rate: {config.lr}")
    print(f"   Epochs: {config.epochs}")
    print(f"   Device: {config.device}")
    print(f"   ç‰©ç†æŸå¤±æƒé‡: {config.lambda_physics}")

    os.makedirs(config.checkpoint_dir, exist_ok=True)

    # æ•°æ®å¤„ç†
    processor = MemoryDataProcessor(
        config.data_path,
        config.seq_len,
        config.pred_len,
        config.max_samples,
        config
    )

    train_dataset = Seq2SeqDataset(processor.train_X, processor.train_ctrl, processor.train_Y)
    val_dataset = Seq2SeqDataset(processor.val_X, processor.val_ctrl, processor.val_Y)

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=True,
        collate_fn=seq2seq_collate_fn
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=True,
        collate_fn=seq2seq_collate_fn
    )

    # æ¨¡å‹
    model = PrinterPINN_Seq2Seq(config)
    if torch.cuda.device_count() > 1:
        print(f"ğŸ® ä½¿ç”¨ {torch.cuda.device_count()} ä¸ª GPU!")
        model = nn.DataParallel(model)
    model = model.to(config.device)

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.lr,
        betas=(0.9, 0.999),
        weight_decay=1e-5
    )

    warmup_scheduler = LinearLR(
        optimizer,
        start_factor=0.1,
        end_factor=1.0,
        total_iters=config.warmup_epochs
    )
    cosine_scheduler = CosineAnnealingLR(
        optimizer,
        T_max=config.epochs - config.warmup_epochs,
        eta_min=1e-6
    )
    scheduler = SequentialLR(
        optimizer,
        schedulers=[warmup_scheduler, cosine_scheduler],
        milestones=[config.warmup_epochs]
    )

    criterion = nn.MSELoss()
    scaler = GradScaler('cuda')

    # TensorBoard
    log_dir = os.path.join("runs", "seq2seq_experiment")
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir)

    # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_val_loss = float('inf')
    
    if config.resume_from is not None and os.path.exists(config.resume_from):
        start_epoch, _, _, best_val_loss, original_batch_size = load_checkpoint(
            model, optimizer, scheduler, config.resume_from, 
            config.load_optimizer_state, verbose=True
        )
        
        # å¤„ç†batch sizeå˜åŒ–çš„æƒ…å†µ
        if original_batch_size and original_batch_size != config.batch_size:
            batch_scale = config.batch_size / original_batch_size
            print(f"\nâš ï¸  æ£€æµ‹åˆ°Batch Sizeå˜åŒ–!")
            print(f"   åŸå§‹: {original_batch_size} -> å½“å‰: {config.batch_size}")
            print(f"   ç¼©æ”¾å› å­: {batch_scale:.2f}x")
            
            # æ ¹æ®batch sizeç¼©æ”¾è°ƒæ•´å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨çŠ¶æ€
            if config.load_optimizer_state:
                print(f"   ğŸ”§ æ£€æµ‹åˆ°åŠ è½½äº†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä½†batch sizeå·²å˜åŒ–")
                print(f"   å»ºè®®è®¾ç½® --load_optimizer_state=False æˆ– --lr {config.lr * batch_scale:.2e}")
                print(f"   æˆ–æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡ä»¥åŒ¹é…æ–°çš„batch size")
            else:
                # è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
                new_lr = config.lr * batch_scale
                for param_group in optimizer.param_groups:
                    param_group['lr'] = new_lr
                print(f"   âœ… å­¦ä¹ ç‡å·²è‡ªåŠ¨è°ƒæ•´: {config.lr:.2e} -> {new_lr:.2e}")
                config.lr = new_lr
            
            # è°ƒæ•´schedulerçš„milestone
            warmup_scheduler.total_iters = max(1, int(config.warmup_epochs))
            cosine_scheduler.T_max = max(1, config.epochs - config.warmup_epochs)
        
        config.start_epoch = start_epoch
        config.original_batch_size = original_batch_size
    else:
        print("â„¹ï¸  ä»å¤´å¼€å§‹è®­ç»ƒ")

    # åˆ›å»ºçŠ¶æ€ç®¡ç†å™¨
    state_manager = TrainingStateManager(config, model, optimizer, scheduler, config.checkpoint_dir)
    state_manager.current_epoch = start_epoch
    state_manager.best_val_loss = best_val_loss

    # æ³¨å†Œä¼˜é›…é€€å‡ºå¤„ç†å™¨
    def graceful_exit_handler(signum=None, frame=None):
        print(f"\n{'='*70}")
        print(f"âš ï¸  æ¥æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
        print(f"{'='*70}")
        
        time_info = state_manager.get_time_info()
        print(f"\nğŸ“Š è®­ç»ƒè¿›åº¦:")
        print(f"   å½“å‰Epoch: {state_manager.current_epoch}/{config.epochs}")
        print(f"   è®­ç»ƒæŸå¤±: {state_manager.train_loss:.6f}")
        print(f"   éªŒè¯æŸå¤±: {state_manager.val_loss:.6f}")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {state_manager.best_val_loss:.6f}")
        print(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:")
        print(f"   å·²è®­ç»ƒæ—¶é—´: {time_info['elapsed_formatted']}")
        print(f"   å¹³å‡æ¯ä¸ªEpoch: {format_time(time_info['avg_epoch_time'])}")
        
        if config.save_on_exit:
            print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹...")
            final_checkpoint_path = os.path.join(
                config.checkpoint_dir, 
                f"interrupted_epoch{state_manager.current_epoch}.pth"
            )
            state_manager.save_checkpoint(final_checkpoint_path)
            
            # ä¿å­˜è®­ç»ƒæ‘˜è¦
            summary_path = os.path.join(config.checkpoint_dir, "training_summary.txt")
            with open(summary_path, 'w') as f:
                f.write("è®­ç»ƒä¸­æ–­æ‘˜è¦\n")
                f.write("="*70 + "\n\n")
                f.write(f"å½“å‰Epoch: {state_manager.current_epoch}/{config.epochs}\n")
                f.write(f"è®­ç»ƒæŸå¤±: {state_manager.train_loss:.6f}\n")
                f.write(f"éªŒè¯æŸå¤±: {state_manager.val_loss:.6f}\n")
                f.write(f"æœ€ä½³éªŒè¯æŸå¤±: {state_manager.best_val_loss:.6f}\n")
                f.write(f"å·²è®­ç»ƒæ—¶é—´: {time_info['elapsed_formatted']}\n")
                f.write(f"é…ç½®:\n")
                for key, value in config.__dict__.items():
                    f.write(f"  {key}: {value}\n")
            print(f"   æ‘˜è¦å·²ä¿å­˜: {summary_path}")
        
        print(f"\n{'='*70}")
        print(f"âœ… ä¼˜é›…å…³é—­å®Œæˆ")
        print(f"{'='*70}\n")
        
        writer.close()
        exit(0)

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, graceful_exit_handler)
    signal.signal(signal.SIGTERM, graceful_exit_handler)
    
    # æ³¨å†Œatexitå¤„ç†å™¨ï¼ˆæ­£å¸¸é€€å‡ºæ—¶ä¹Ÿä¼šè°ƒç”¨ï¼‰
    atexit.register(lambda: None)  # é˜²æ­¢é‡å¤æ³¨å†Œ

    # è®­ç»ƒå¾ªç¯
    print_every = 50
    epoch_times = []
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...\n")
    print(f"{'='*70}")
    
    for epoch in range(start_epoch, config.epochs):
        epoch_start = time.time()
        model.train()
        epoch_loss = 0
        optimizer.zero_grad()

        for batch_idx, (batch_x_hist, batch_x_ctrl, batch_y) in enumerate(train_loader):
            batch_x_hist = batch_x_hist.to(config.device)
            batch_x_ctrl = batch_x_ctrl.to(config.device)
            batch_y = batch_y.to(config.device)

            with autocast('cuda'):
                outputs = model(batch_x_hist, batch_x_ctrl)
                
                data_loss = criterion(outputs, batch_y)
                
                if isinstance(model, nn.DataParallel):
                    physics_loss = model.module.physics_loss(outputs, batch_y)
                else:
                    physics_loss = model.physics_loss(outputs, batch_y)
                
                total_loss = data_loss + config.lambda_physics * physics_loss

            scaler.scale(total_loss).backward()
            
            if (batch_idx + 1) % config.gradient_accumulation_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            epoch_loss += total_loss.item() * config.gradient_accumulation_steps

            if (batch_idx + 1) % print_every == 0:
                avg_so_far = epoch_loss / (batch_idx + 1)
                time_info = state_manager.get_time_info()
                
                print(f"  ğŸ”µ Epoch {epoch+1:3d}/{config.epochs} | "
                      f"Batch {batch_idx+1:5d}/{len(train_loader):5d} | "
                      f"Loss: {avg_so_far:.6f} | LR: {optimizer.param_groups[0]['lr']:.6e} | "
                      f"ETA: {time_info['eta_formatted']}")

        avg_train_loss = epoch_loss / len(train_loader)
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_x_hist, batch_x_ctrl, batch_y in val_loader:
                batch_x_hist = batch_x_hist.to(config.device)
                batch_x_ctrl = batch_x_ctrl.to(config.device)
                batch_y = batch_y.to(config.device)
                
                with autocast('cuda'):
                    outputs = model(batch_x_hist, batch_x_ctrl)
                    loss = criterion(outputs, batch_y)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        scheduler.step()

        epoch_time = time.time() - epoch_start
        epoch_times.append(epoch_time)
        
        # æ›´æ–°çŠ¶æ€ç®¡ç†å™¨
        state_manager.update_epoch(epoch + 1, avg_train_loss, avg_val_loss)
        time_info = state_manager.get_time_info()
        
        # æ‰“å°epochæ‘˜è¦
        print(f"ğŸŸ¢ Epoch {epoch+1:3d}/{config.epochs} ({time_info['progress_percent']:5.1f}%) | "
              f"Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f} | "
              f"Time: {epoch_time:.2f}s | ETA: {time_info['eta_formatted']}")

        writer.add_scalar("Loss/train", avg_train_loss, epoch)
        writer.add_scalar("Loss/val", avg_val_loss, epoch)
        writer.add_scalar("Time/epoch", epoch_time, epoch)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            checkpoint_filename = "best_seq2seq_model.pth"
            save_checkpoint(epoch+1, model, optimizer, scheduler, avg_train_loss, avg_val_loss, 
                           best_val_loss, config, os.path.join(config.checkpoint_dir, checkpoint_filename))
            print(f"  ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % config.save_interval == 0:
            checkpoint_filename = f"checkpoint_epoch{epoch+1}.pth"
            save_checkpoint(epoch+1, model, optimizer, scheduler, avg_train_loss, avg_val_loss, 
                           best_val_loss, config, os.path.join(config.checkpoint_dir, checkpoint_filename))

    total_time = time.time() - state_manager.training_start_time
    print(f"\n{'='*70}")
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*70}")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {format_time(total_time)}")
    print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
    print(f"{'='*70}\n")
    
    writer.close()

def get_args():
    parser = argparse.ArgumentParser(description='è®­ç»ƒ3Dæ‰“å°æœºPINNæ¨¡å‹')
    parser.add_argument('--data_path', type=str, default='enterprise_dataset/printer_enterprise_data.csv', 
                        help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=30, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=256, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=2e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--resume_from', type=str, help='ä»æŒ‡å®šæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints_seq2seq', help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    parser.add_argument('--save_on_exit', type=bool, default=True, help='é€€å‡ºæ—¶æ˜¯å¦ä¿å­˜æƒé‡')
    parser.add_argument('--save_interval', type=int, default=5, help='å®šæœŸä¿å­˜é—´éš”')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu', help='è®¾å¤‡')
    parser.add_argument('--load_optimizer_state', type=bool, default=True, 
                        help='åŠ è½½æ£€æŸ¥ç‚¹æ—¶æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆbatch sizeå˜åŒ–æ—¶å»ºè®®ä¸ºFalseï¼‰')
    return parser.parse_args()

if __name__ == "__main__":
    args = get_args()
    config = Config()
    
    # æ›´æ–°é…ç½®å‚æ•°
    config.data_path = args.data_path
    config.epochs = args.epochs
    config.batch_size = args.batch_size
    config.lr = args.lr
    config.resume_from = args.resume_from
    config.checkpoint_dir = args.checkpoint_dir
    config.save_on_exit = args.save_on_exit
    config.save_interval = args.save_interval
    config.device = args.device
    config.load_optimizer_state = args.load_optimizer_state
    
    train_pinn_seq2seq(config)
