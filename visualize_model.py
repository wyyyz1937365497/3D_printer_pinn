# evaluate_pinn.py
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import sys
from train_pinn_seq2seq import (
    Config, PrinterPINN_Seq2Seq, MemoryDataProcessor, 
    PositionalEncoding, seq2seq_collate_fn
)
from torch.utils.data import DataLoader

def visualize_predictions(preds, targets, feature_names, save_path):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
    fig, axes = plt.subplots(len(feature_names), 1, figsize=(12, 3*len(feature_names)))
    
    for i, (ax, name) in enumerate(zip(axes, feature_names)):
        # ç»˜åˆ¶ç¬¬ä¸€æ¡æ ·æœ¬çš„é¢„æµ‹
        pred_line = ax.plot(preds[0, :, i], label='Prediction', linestyle='--', linewidth=2)
        true_line = ax.plot(targets[0, :, i], label='Ground Truth', alpha=0.7, linewidth=2)
        
        # æ·»åŠ ç½®ä¿¡åŒºé—´ (ç®€å•çš„æ ‡å‡†å·®)
        std = np.std(preds[:, :, i], axis=0)
        ax.fill_between(range(len(preds[0, :, i])), 
                       preds[0, :, i] - std, 
                       preds[0, :, i] + std, 
                       alpha=0.2, color=pred_line[0].get_color())
        
        ax.set_title(f'{name}', fontsize=12, fontweight='bold')
        ax.set_ylabel('Value')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.xlabel('Time Step (Future)')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
    plt.close()

def calculate_metrics(preds, targets, feature_names):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    
    metrics = {}
    batch_size, seq_len, n_features = preds.shape
    
    for i, name in enumerate(feature_names):
        pred_flat = preds[:, :, i].flatten()
        target_flat = targets[:, :, i].flatten()
        
        mse = mean_squared_error(target_flat, pred_flat)
        mae = mean_absolute_error(target_flat, pred_flat)
        r2 = r2_score(target_flat, pred_flat)
        
        metrics[name] = {
            'MSE': mse,
            'MAE': mae,
            'R2': r2,
            'RMSE': np.sqrt(mse)
        }
    
    return metrics

def fault_detection_analysis(preds, targets, threshold=2.0):
    """ç®€å•çš„æ•…éšœæ£€æµ‹åˆ†æ"""
    errors = np.abs(preds - targets)
    max_errors = np.max(errors, axis=1)  # [batch_size, n_features]
    
    # æ ‡è®°å¼‚å¸¸ (è¯¯å·®è¶…è¿‡é˜ˆå€¼å€çš„æ ‡å‡†å·®)
    mean_errors = np.mean(errors, axis=1)
    std_errors = np.std(errors, axis=1)
    
    anomalies = (errors > mean_errors[:, np.newaxis, :] + threshold * std_errors[:, np.newaxis, :])
    
    return anomalies, max_errors

def evaluate_model(config_path, model_path, num_samples=100):
    """è¯„ä¼°æ¨¡å‹"""
    print("=" * 70)
    print("ğŸ” è¯„ä¼° Seq2Seq æ¨¡å‹")
    print("=" * 70)
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“‚ åŠ è½½é…ç½®: {config_path}")
    checkpoint = torch.load(model_path, map_location='cpu')
    config = Config()
    config.__dict__.update(checkpoint['config'])
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('evaluation_results', exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    processor = MemoryDataProcessor(
        config.data_path,
        config.seq_len,
        config.pred_len,
        config.max_samples,
        config
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    val_dataset = type('Dataset', (), {
        '__len__': lambda self: len(processor.val_X),
        '__getitem__': lambda self, idx: (
            processor.val_X[idx], 
            processor.val_ctrl[idx], 
            processor.val_Y[idx]
        )
    })()
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=32,
        shuffle=False,
        num_workers=2,
        collate_fn=seq2seq_collate_fn
    )
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ¤– åŠ è½½æ¨¡å‹...")
    model = PrinterPINN_Seq2Seq(config)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(config.device)
    model.eval()
    
    # é¢„æµ‹
    print("ğŸ”® è¿›è¡Œé¢„æµ‹...")
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for batch_x_hist, batch_x_ctrl, batch_y in val_loader:
            batch_x_hist = batch_x_hist.to(config.device)
            batch_x_ctrl = batch_x_ctrl.to(config.device)
            
            outputs = model(batch_x_hist, batch_x_ctrl)
            
            all_preds.append(outputs.cpu().numpy())
            all_targets.append(batch_y.numpy())
    
    # åˆå¹¶ç»“æœ
    preds = np.concatenate(all_preds, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    
    # åå½’ä¸€åŒ–
    preds_real = processor.inverse_transform_y(preds)
    targets_real = processor.inverse_transform_y(targets)
    
    # è®¡ç®—æŒ‡æ ‡
    print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    metrics = calculate_metrics(preds_real, targets_real, config.state_cols)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š è¯„ä¼°ç»“æœ")
    print("=" * 70)
    
    for feature, metric_dict in metrics.items():
        print(f"\n{feature}:")
        print(f"  MSE:  {metric_dict['MSE']:.6f}")
        print(f"  RMSE: {metric_dict['RMSE']:.6f}")
        print(f"  MAE:  {metric_dict['MAE']:.6f}")
        print(f"  RÂ²:   {metric_dict['R2']:.6f}")
    
    # æ•…éšœæ£€æµ‹åˆ†æ
    print("\n" + "=" * 70)
    print("ğŸ”§ æ•…éšœæ£€æµ‹åˆ†æ")
    print("=" * 70)
    
    anomalies, max_errors = fault_detection_analysis(preds_real, targets_real)
    
    print(f"å¼‚å¸¸æ ·æœ¬æ•° (é˜ˆå€¼=2Ïƒ): {np.any(anomalies, axis=(1,2)).sum()} / {len(anomalies)}")
    
    for i, feature in enumerate(config.state_cols):
        feature_anomalies = np.any(anomalies[:, :, i], axis=1)
        print(f"{feature}: {feature_anomalies.sum()} å¼‚å¸¸æ—¶é—´æ­¥")
    
    # å¯è§†åŒ–
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
    visualize_path = 'evaluation_results/prediction_comparison.png'
    visualize_predictions(preds_real, targets_real, config.state_cols, visualize_path)
    
    # ä¿å­˜ç»“æœ
    results_path = 'evaluation_results/metrics.txt'
    with open(results_path, 'w') as f:
        f.write("Model Evaluation Results\n")
        f.write("=" * 70 + "\n\n")
        
        for feature, metric_dict in metrics.items():
            f.write(f"{feature}:\n")
            for metric_name, value in metric_dict.items():
                f.write(f"  {metric_name}: {value:.6f}\n")
            f.write("\n")
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {results_path}")
    print("=" * 70)

if __name__ == "__main__":
    # æŒ‡å®šæ¨¡å‹è·¯å¾„
    model_path = "checkpoints_seq2seq/best_seq2seq_model.pth"
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ train_pinn_seq2seq.py")
        sys.exit(1)
    
    # å¼€å§‹è¯„ä¼°
    evaluate_model(None, model_path, num_samples=100)
