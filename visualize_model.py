# visualize_model.py
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import sys
from collections import OrderedDict
from torch.utils.data import DataLoader
import sklearn.metrics as metrics

# ä»è®­ç»ƒè„šæœ¬å¯¼å…¥å¿…è¦çš„ç±»
# ç¡®ä¿åŒçº§ç›®å½•ä¸‹æœ‰ train_pinn_seq2seq.py
from train_pinn_seq2seq import (
    Config, PrinterPINN_Seq2Seq, MemoryDataProcessor, 
    PositionalEncoding, seq2seq_collate_fn
)

# ==================== è¯„ä¼°ä¸å¯è§†åŒ–å‡½æ•° ====================

def load_model_checkpoint(model_path, model, device='cpu'):
    """
    åŠ è½½æ¨¡å‹æƒé‡ï¼Œè‡ªåŠ¨å¤„ç† DataParallel çš„ 'module.' å‰ç¼€é—®é¢˜
    """
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {model_path}")
    checkpoint = torch.load(model_path, map_location=device)
    state_dict = checkpoint['model_state_dict']
    
    # åˆ›å»ºä¸€ä¸ªæ–°çš„ state_dictï¼Œå»æ‰ 'module.' å‰ç¼€
    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k[7:] if k.startswith('module.') else k # ç§»é™¤ 'module.'
        new_state_dict[name] = v
    
    # åŠ è½½ä¿®æ­£åçš„æƒé‡
    model.load_state_dict(new_state_dict)
    print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼ˆå·²å¤„ç† DataParallel å‰ç¼€ï¼‰")
    
    # è¿”å›é…ç½®ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    config_dict = checkpoint.get('config', {})
    return config_dict

def visualize_predictions(preds, targets, feature_names, save_path, processor):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœï¼Œå¤„ç†ä¸åŒé‡çº²"""
    fig, axes = plt.subplots(len(feature_names), 1, figsize=(14, 2.5*len(feature_names)))
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªç‰¹å¾ï¼Œaxesä¸æ˜¯æ•°ç»„ï¼Œéœ€è½¬æ¢
    if len(feature_names) == 1:
        axes = [axes]

    for i, (ax, name) in enumerate(zip(axes, feature_names)):
        # è·å–çœŸå®èŒƒå›´ç”¨äºç»˜å›¾
        true_vals = targets[0, :, i]
        pred_vals = preds[0, :, i]
        
        # ç»˜åˆ¶
        ax.plot(true_vals, label='Ground Truth', alpha=0.8, linewidth=2, color='tab:blue')
        ax.plot(pred_vals, label='Prediction', linestyle='--', linewidth=2, color='tab:orange')
        
        # è®¡ç®—è¯¯å·®å¹¶å¡«å……
        error = np.abs(pred_vals - true_vals)
        # ax.fill_between(range(len(pred_vals)), pred_vals - error, pred_vals + error, alpha=0.2, color='tab:orange')
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title(f'{name}', fontsize=12, fontweight='bold')
        ax.set_ylabel('Value')
        ax.legend(loc='upper right')
        ax.grid(True, alpha=0.3)
        
        # è®¡ç®—è¯¥æŒ‡æ ‡çš„å¹³å‡ç»å¯¹è¯¯å·®å¹¶æ˜¾ç¤ºåœ¨å›¾ä¸Š
        mae = np.mean(np.abs(preds[:, :, i] - targets[:, :, i]))
        ax.text(0.02, 0.9, f'MAE: {mae:.4f}', transform=ax.transAxes, 
                bbox=dict(facecolor='white', alpha=0.7))
    
    plt.xlabel('Time Step (Future)', fontsize=12)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
    plt.close()

def calculate_metrics(preds, targets, feature_names):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    metrics_results = {}
    batch_size, seq_len, n_features = preds.shape
    
    for i, name in enumerate(feature_names):
        pred_flat = preds[:, :, i].flatten()
        target_flat = targets[:, :, i].flatten()
        
        # å¿½ç•¥ NaN æˆ– Inf
        mask = np.isfinite(pred_flat) & np.isfinite(target_flat)
        if mask.sum() == 0:
            print(f"âš ï¸  ç‰¹å¾ {name} åŒ…å«æ— æ•ˆæ•°æ®ï¼Œè·³è¿‡è¯„ä¼°ã€‚")
            continue

        mse = metrics.mean_squared_error(target_flat[mask], pred_flat[mask])
        mae = metrics.mean_absolute_error(target_flat[mask], pred_flat[mask])
        
        # R2 score å¯èƒ½ä¸ºè´Ÿï¼Œè¯´æ˜æ¨¡å‹æå·®
        try:
            r2 = metrics.r2_score(target_flat[mask], pred_flat[mask])
        except:
            r2 = -999.0
            
        metrics_results[name] = {
            'MSE': mse,
            'MAE': mae,
            'RMSE': np.sqrt(mse),
            'R2': r2
        }
    
    return metrics_results

def evaluate_model(config_path, model_path, num_samples=100):
    """ä¸»è¯„ä¼°å‡½æ•°"""
    print("=" * 70)
    print("ğŸ” è¯„ä¼° Seq2Seq æ¨¡å‹")
    print("=" * 70)
    
    # 1. åŠ è½½æ£€æŸ¥ç‚¹ä»¥è·å–é…ç½®
    # ä¸ºäº†åŠ è½½é…ç½®ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæœ‰ä¸€ä¸ªä¸´æ—¶çš„ Config å¯¹è±¡
    # å®é™…ä¸Š Config æ˜¯å®šä¹‰åœ¨ train è„šæœ¬é‡Œçš„ï¼Œæˆ‘ä»¬ç›´æ¥å®ä¾‹åŒ–å³å¯
    temp_config = Config()
    
    try:
        loaded_config = load_model_checkpoint(model_path, PrinterPINN_Seq2Seq(temp_config))
        # å¦‚æœæ£€æŸ¥ç‚¹é‡Œæœ‰ä¿å­˜çš„é…ç½®ï¼Œæ›´æ–°å½“å‰é…ç½®
        if loaded_config:
            # å°†å­—å…¸æ›´æ–°åˆ° Config å¯¹è±¡ä¸­
            for k, v in loaded_config.items():
                if hasattr(temp_config, k):
                    setattr(temp_config, k, v)
        print(f"âœ… å·²ä»æ£€æŸ¥ç‚¹æ¢å¤é…ç½®å‚æ•°")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•ä»æ£€æŸ¥ç‚¹æ¢å¤é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
    
    # 2. å‡†å¤‡è¾“å‡ºç›®å½•
    os.makedirs('evaluation_results', exist_ok=True)
    
    # 3. åŠ è½½æ•°æ® (ä½¿ç”¨æ¢å¤çš„é…ç½®)
    print("\nğŸ“Š åŠ è½½å¹¶å¤„ç†æ•°æ®...")
    try:
        processor = MemoryDataProcessor(
            temp_config.data_path,
            temp_config.seq_len,
            temp_config.pred_len,
            temp_config.max_samples,
            temp_config
        )
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("æç¤º: ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®ä¸”åˆ—åä¸è®­ç»ƒæ—¶ä¸€è‡´ã€‚")
        return

    # åˆ›å»ºéªŒè¯é›† Loader
    # å®šä¹‰ä¸€ä¸ªç®€å•çš„ Dataset åŒ…è£…å™¨ï¼Œé¿å…é‡æ–°å®šä¹‰ç±»
    val_dataset = type('Dataset', (), {
        '__len__': lambda self: len(processor.val_X),
        '__getitem__': lambda self, idx: (
            processor.val_X[idx], 
            processor.val_ctrl[idx], 
            processor.val_Y[idx]
        )
    })()
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=32, # è¯„ä¼°æ—¶ batch size å¯ä»¥å°ä¸€ç‚¹ï¼Œé˜²æ­¢ OOM
        shuffle=False,
        num_workers=0,
        collate_fn=seq2seq_collate_fn
    )
    
    # 4. åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹...")
    model = PrinterPINN_Seq2Seq(temp_config)
    model = model.to(temp_config.device)
    
    # å…³é”®æ­¥éª¤ï¼šåŠ è½½æƒé‡ï¼ˆå»æ‰ module.ï¼‰
    load_model_checkpoint(model_path, model, temp_config.device)
    
    model.eval()
    
    # 5. æ‰§è¡Œé¢„æµ‹
    print("ğŸ”® å¼€å§‹é¢„æµ‹éªŒè¯é›†...")
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for batch_x_hist, batch_x_ctrl, batch_y in val_loader:
            batch_x_hist = batch_x_hist.to(temp_config.device)
            batch_x_ctrl = batch_x_ctrl.to(temp_config.device)
            
            outputs = model(batch_x_hist, batch_x_ctrl)
            
            all_preds.append(outputs.cpu().numpy())
            all_targets.append(batch_y.numpy())
    
    # åˆå¹¶ç»“æœ
    preds = np.concatenate(all_preds, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    
    # 6. åå½’ä¸€åŒ–
    print("ğŸ“ˆ åå½’ä¸€åŒ–æ•°æ®...")
    preds_real = processor.inverse_transform_y(preds)
    targets_real = processor.inverse_transform_y(targets)
    
    # 7. è®¡ç®—æŒ‡æ ‡
    print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    metrics_res = calculate_metrics(preds_real, targets_real, temp_config.state_cols)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š è¯„ä¼°ç»“æœ")
    print("=" * 70)
    
    for feature, metric_dict in metrics_res.items():
        print(f"\nã€{feature}ã€‘")
        print(f"  MSE:  {metric_dict['MSE']:.6f}")
        print(f"  RMSE: {metric_dict['RMSE']:.6f}")
        print(f"  MAE:  {metric_dict['MAE']:.6f}")
        print(f"  RÂ²:   {metric_dict['R2']:.6f}")
    
    # 8. å¯è§†åŒ–
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    visualize_path = 'evaluation_results/prediction_visualization.png'
    # åªå–å‰ num_samples ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–ï¼Œé¿å…å›¾è¡¨è¿‡äºå¯†é›†
    visualize_predictions(
        preds_real[:num_samples], 
        targets_real[:num_samples], 
        temp_config.state_cols, 
        visualize_path,
        processor
    )
    
    # 9. ä¿å­˜ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶
    results_path = 'evaluation_results/metrics_report.txt'
    with open(results_path, 'w') as f:
        f.write("Model Evaluation Report\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"Model Path: {model_path}\n")
        f.write(f"Samples Evaluated: {len(preds_real)}\n\n")
        
        for feature, metric_dict in metrics_res.items():
            f.write(f"Feature: {feature}\n")
            for metric_name, value in metric_dict.items():
                f.write(f"  {metric_name}: {value:.6f}\n")
            f.write("\n")
            
    print(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {results_path}")
    print("=" * 70)
    print("âœ… è¯„ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    # é»˜è®¤æ¨¡å‹è·¯å¾„
    # å¦‚æœä½ åœ¨è®­ç»ƒä¸­ä½¿ç”¨äº† DataParallelï¼Œè¿™é‡Œä¼šè‡ªåŠ¨å¤„ç†
    model_path = "checkpoints_seq2seq/best_seq2seq_model.pth"
    
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·æ£€æŸ¥ train_pinn_seq2seq.py ä¸­çš„ checkpoint_dir è®¾ç½®")
        sys.exit(1)
    
    # å¼€å§‹è¯„ä¼°
    evaluate_model(None, model_path, num_samples=100)
