import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import os
import matplotlib.pyplot as plt
import time

# ==================== é…ç½®å‚æ•° ====================
config = {
    'data_path': 'enterprise_dataset/printer_enterprise_data.csv',
    'cache_path': 'enterprise_dataset/processed_data.pt',  # æ•°æ®ç¼“å­˜è·¯å¾„
    'seq_len': 200,          # åºåˆ—é•¿åº¦
    'batch_size': 512,       # æ‰¹æ¬¡å¤§å°
    'hidden_dim': 128,       # LSTMéšè—å±‚ç»´åº¦
    'tcn_channels': [64, 64, 128], # TCNå„å±‚é€šé“æ•°
    'lr': 1e-3,
    'epochs': 50,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'num_workers': 4,
    'test_mode': True,      # â­ æµ‹è¯•æ¨¡å¼ï¼šåªåŠ è½½å°‘é‡æ•°æ®å¿«é€ŸéªŒè¯
    'test_samples': 1000,    # æµ‹è¯•æ¨¡å¼ä½¿ç”¨çš„æ ·æœ¬æ•°
    'use_cache': True,      # æ˜¯å¦ä½¿ç”¨ç¼“å­˜
}

# ==================== 1. æ•°æ®é¢„å¤„ç†æ¨¡å— ====================

class PrinterDataProcessor:
    def __init__(self, data_path, seq_len, use_cache=True, test_mode=False, test_samples=1000):
        self.data_path = data_path
        self.seq_len = seq_len
        self.use_cache = use_cache
        self.test_mode = test_mode
        self.test_samples = test_samples
        
        # å®šä¹‰è¾“å…¥å’Œè¾“å‡ºåˆ—
        self.input_cols = ['ctrl_T_target', 'ctrl_speed_set', 'ctrl_heater_base']
        self.target_cols = ['temperature_C', 'vibration_disp_m', 'vibration_vel_m_s', 
                           'motor_current_A', 'pressure_bar', 'acoustic_signal']
        
        # æ£€æŸ¥ç¼“å­˜
        cache_file = 'enterprise_dataset/processed_data.pt'
        if use_cache and os.path.exists(cache_file):
            print(f"ğŸ“¦ å‘ç°ç¼“å­˜æ–‡ä»¶ {cache_file}ï¼Œç›´æ¥åŠ è½½...")
            self.load_from_cache(cache_file)
        else:
            print(f"ğŸ”„ ç¼“å­˜ä¸å­˜åœ¨æˆ–ç¦ç”¨ï¼Œå¼€å§‹å¤„ç†åŸå§‹æ•°æ®...")
            self.prepare_data()
            if use_cache:
                self.save_to_cache(cache_file)
    
    def prepare_data(self):
        """å¤„ç†åŸå§‹æ•°æ®å¹¶åˆ†å‰²"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {self.data_path}")
        df = pd.read_csv(self.data_path)
        print(f"âœ… åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
        
        # =============================================================
        # ğŸ”§ ä¿®å¤æ ¸å¿ƒ: å¤„ç† NaN å’Œ Inf å€¼
        # =============================================================
        print("ğŸ” æ£€æŸ¥å¼‚å¸¸å€¼")
        
        # 1. ç»Ÿè®¡å¼‚å¸¸å€¼
        nan_counts = df.isna().sum().sum()
        # æ£€æŸ¥æ— ç©·å¤§
        inf_counts = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()
        
        print(f"   å‘ç° NaN: {nan_counts}, Inf: {inf_counts}")
        
        # 2. å°†æ— ç©·å¤§æ›¿æ¢ä¸º NaN (ä»¥ä¾¿ç»Ÿä¸€å¤„ç†)
        # replace inf with nan
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        
        # 3. ä½¿ç”¨æ–°ç‰ˆè¯­æ³•è¿›è¡Œå¡«å…… (è§£å†³ FutureWarning)
        # ffill: forward fill, bfill: backward fill
        if df.isna().sum().sum() > 0:
            print("âš ï¸ æ­£åœ¨æ¸…ç†å¼‚å¸¸å€¼å¹¶æ’å€¼...")
            df = df.ffill().bfill()
            
            # å†æ¬¡æ£€æŸ¥ï¼šå¦‚æœæ•´åˆ—éƒ½æ˜¯å¼‚å¸¸å€¼ï¼Œç›´æ¥åˆ é™¤è¯¥åˆ—ï¼ˆä¸å¤ªå¯èƒ½ï¼Œä½†ä¸ºäº†å¥å£®æ€§ï¼‰
            df.dropna(axis=1, how='all', inplace=True)
            print("âœ… å¼‚å¸¸å€¼å¤„ç†å®Œæˆ")
        # =============================================================
        
        # æŒ‰æœºå™¨IDåˆ†ç»„å¤„ç†
        X_list = []
        Y_list = []
        
        grouped = df.groupby('machine_id')
        print(f"ğŸ”„ å¤„ç† {len(grouped)} å°æœºå™¨çš„æ•°æ®...")
        
        start_time = time.time()
        for idx, (machine_id, group) in enumerate(grouped):
            if idx % 10 == 0:
                print(f"   è¿›åº¦: {idx}/{len(grouped)} æœºå™¨")
            
            group = group.sort_values('timestamp').reset_index(drop=True)
            
            # æå–æ•°æ®
            X_raw = group[self.input_cols].values
            Y_raw = group[self.target_cols].values
            
            # æ»‘åŠ¨çª—å£
            total_len = len(group)
            if total_len < self.seq_len + 1:
                continue
                
            # è¿™é‡Œå¢åŠ ä¸€ä¸ªå®‰å…¨æ£€æŸ¥ï¼šå¦‚æœ slice ä¸­è¿˜æœ‰ NaN (è™½ç„¶å‰é¢å¤„ç†è¿‡)ï¼Œè·³è¿‡
            if np.isnan(X_raw).any() or np.isnan(Y_raw).any():
                continue
                
            for i in range(total_len - self.seq_len):
                X_list.append(X_raw[i:i+self.seq_len])
                Y_list.append(Y_raw[i+self.seq_len])
        
        self.X_seq = np.array(X_list, dtype=np.float32)
        self.Y_seq = np.array(Y_list, dtype=np.float32)
        
        elapsed = time.time() - start_time
        print(f"âœ… åºåˆ—ç”Ÿæˆå®Œæˆ: {len(self.X_seq)} ä¸ªåºåˆ—ï¼Œè€—æ—¶ {elapsed:.2f}s")
        
        if len(self.X_seq) == 0:
            raise ValueError("æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆåºåˆ—ï¼è¯·æ£€æŸ¥æ•°æ®æ˜¯å¦å…¨éƒ¨ä¸ºç©ºã€‚")
        
        # æµ‹è¯•æ¨¡å¼ï¼šåªå–å°‘é‡æ•°æ®
        if self.test_mode:
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªä½¿ç”¨å‰ {self.test_samples} ä¸ªæ ·æœ¬")
            self.X_seq = self.X_seq[:self.test_samples]
            self.Y_seq = self.Y_seq[:self.test_samples]
        
        # å½’ä¸€åŒ–
        print("ğŸ“Š å¼€å§‹å½’ä¸€åŒ–...")
        n_samples, t_steps, n_features = self.X_seq.shape
        X_flat = self.X_seq.reshape(-1, n_features)
        
        self.scaler_X = StandardScaler()
        self.X_seq = self.scaler_X.fit_transform(X_flat).reshape(n_samples, t_steps, n_features)
        
        self.scaler_Y = StandardScaler()
        self.Y_seq = self.scaler_Y.fit_transform(self.Y_seq)
        
        # åˆ’åˆ†æ•°æ®é›†
        split_idx = int(len(self.X_seq) * 0.8)
        self.train_X = self.X_seq[:split_idx]
        self.train_Y = self.Y_seq[:split_idx]
        self.val_X = self.X_seq[split_idx:]
        self.val_Y = self.Y_seq[split_idx:]
        
        print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(self.train_X)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(self.val_X)} æ ·æœ¬")
    
    def save_to_cache(self, cache_path):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°ç¼“å­˜"""
        print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜åˆ° {cache_path}...")
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        
        cache_data = {
            'train_X': self.train_X,
            'train_Y': self.train_Y,
            'val_X': self.val_X,
            'val_Y': self.val_Y,
            'scaler_X_mean': self.scaler_X.mean_,
            'scaler_X_scale': self.scaler_X.scale_,
            'scaler_Y_mean': self.scaler_Y.mean_,
            'scaler_Y_scale': self.scaler_Y.scale_,
            'input_cols': self.input_cols,
            'target_cols': self.target_cols,
        }
        
        torch.save(cache_data, cache_path)
        file_size = os.path.getsize(cache_path) / 1024 / 1024  # MB
        print(f"âœ… ç¼“å­˜ä¿å­˜æˆåŠŸ! æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    
    def load_from_cache(self, cache_path):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        cache_data = torch.load(cache_path)
        
        self.train_X = cache_data['train_X']
        self.train_Y = cache_data['train_Y']
        self.val_X = cache_data['val_X']
        self.val_Y = cache_data['val_Y']
        
        # é‡å»ºscaler
        self.scaler_X = StandardScaler()
        self.scaler_X.mean_ = cache_data['scaler_X_mean']
        self.scaler_X.scale_ = cache_data['scaler_X_scale']
        
        self.scaler_Y = StandardScaler()
        self.scaler_Y.mean_ = cache_data['scaler_Y_mean']
        self.scaler_Y.scale_ = cache_data['scaler_Y_scale']
        
        self.input_cols = cache_data['input_cols']
        self.target_cols = cache_data['target_cols']
        
        print(f"âœ… ç¼“å­˜åŠ è½½æˆåŠŸ!")
        print(f"   è®­ç»ƒé›†: {len(self.train_X)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(self.val_X)} æ ·æœ¬")

class PrinterDataset(Dataset):
    def __init__(self, X, Y):
        self.X = torch.from_numpy(X)
        self.Y = torch.from_numpy(Y)
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.Y[idx]

# ==================== 2. ä¿®å¤åçš„ TCN æ¨¡å‹ ====================

class TemporalBlock(nn.Module):
    """ä¿®å¤åçš„ TCN åŸºç¡€å—"""
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super(TemporalBlock, self).__init__()
        
        # ä½¿ç”¨æ­£ç¡®çš„ padding = (kernel_size - 1) * dilation
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,
                              stride=stride, padding=padding, dilation=dilation)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        
        # 1x1 å·ç§¯ç”¨äºåŒ¹é…ç»´åº¦
        self.net = nn.Sequential(self.conv1, self.relu1, self.dropout1)
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        self.kernel_size = kernel_size
        self.dilation = dilation
        
    def forward(self, x):
        # x shape: [Batch, Channel, Time]
        out = self.net(x)
        
        # ç§»é™¤paddingä»¥ä¿æŒå› æœæ€§
        # padding = (kernel_size - 1) * dilation
        # æˆ‘ä»¬éœ€è¦ç§»é™¤æœ€åçš„paddingä¸ªæ—¶é—´æ­¥
        pad = (self.kernel_size - 1) * self.dilation
        out = out[:, :, :-pad]
        
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)

class TCN(nn.Module):
    """ä¿®å¤åçš„æ—¶é—´å·ç§¯ç½‘ç»œ"""
    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):
        super(TCN, self).__init__()
        layers = []
        num_levels = len(num_channels)
        
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            padding = (kernel_size - 1) * dilation_size  # å› æœå·ç§¯çš„padding
            
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,
                                   dilation=dilation_size, padding=padding, dropout=dropout)]
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        # x: [Batch, Seq_Len, Features] -> [Batch, Features, Seq_Len]
        x = x.transpose(1, 2)
        out = self.network(x)
        # out: [Batch, Channels, Seq_Len] -> [Batch, Seq_Len, Channels]
        return out.transpose(1, 2)

class TCNLSTMModel(nn.Module):
    def __init__(self, input_dim, tcn_channels, hidden_dim, output_dim):
        super(TCNLSTMModel, self).__init__()
        
        self.tcn = TCN(input_dim, tcn_channels)
        tcn_output_dim = tcn_channels[-1]
        
        self.lstm = nn.LSTM(tcn_output_dim, hidden_dim, num_layers=2, 
                           batch_first=True, dropout=0.1, bidirectional=False)
        
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x):
        # x: [Batch, Seq_Len, Input_Dim]
        tcn_out = self.tcn(x)  # [Batch, Seq_Len, TCN_Channels]
        lstm_out, (h_n, c_n) = self.lstm(tcn_out) 
        last_step_out = lstm_out[:, -1, :]  # [Batch, Hidden_Dim]
        prediction = self.fc(last_step_out)  # [Batch, Output_Dim]
        
        return prediction

# ==================== 3. è®­ç»ƒä¸è¯„ä¼° ====================

def train_model():
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("="*60)
    print("ğŸš€ TCN-LSTM è®­ç»ƒå¼€å§‹")
    print("="*60)
    print(f"æµ‹è¯•æ¨¡å¼: {'âœ… å¯ç”¨ (å¿«é€ŸéªŒè¯)' if config['test_mode'] else 'âŒ ç¦ç”¨ (å®Œæ•´è®­ç»ƒ)'}")
    print(f"ä½¿ç”¨ç¼“å­˜: {'âœ… å¯ç”¨' if config['use_cache'] else 'âŒ ç¦ç”¨'}")
    print(f"è®¾å¤‡: {config['device']}")
    print(f"æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print("="*60)
    print()
    
    # 1. å‡†å¤‡æ•°æ®
    if not os.path.exists(config['data_path']):
        print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ {config['data_path']}")
        print("è¯·å…ˆè¿è¡Œ MATLAB è„šæœ¬ç”Ÿæˆæ•°æ®")
        return

    processor = PrinterDataProcessor(
        config['data_path'], 
        config['seq_len'],
        use_cache=config['use_cache'],
        test_mode=config['test_mode'],
        test_samples=config['test_samples']
    )
    
    train_dataset = PrinterDataset(processor.train_X, processor.train_Y)
    val_dataset = PrinterDataset(processor.val_X, processor.val_Y)
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], 
                             shuffle=True, num_workers=config['num_workers'], pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], 
                           shuffle=False, num_workers=config['num_workers'], pin_memory=True)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    input_dim = len(processor.input_cols)
    output_dim = len(processor.target_cols)
    
    model = TCNLSTMModel(input_dim, config['tcn_channels'], config['hidden_dim'], output_dim)
    
    if torch.cuda.device_count() > 1:
        print(f"ğŸ® ä½¿ç”¨ {torch.cuda.device_count()} ä¸ª GPU!")
        model = nn.DataParallel(model)
        
    model = model.to(config['device'])
    
    # 3. ä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•°
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    criterion = nn.MSELoss()
    
    # 4. è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    print("="*60)
    
    for epoch in range(config['epochs']):
        epoch_start = time.time()
        
        # è®­ç»ƒ
        model.train()
        epoch_loss = 0
        
        for batch_idx, (batch_X, batch_Y) in enumerate(train_loader):
            batch_X, batch_Y = batch_X.to(config['device']), batch_Y.to(config['device'])
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_Y)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_train_loss = epoch_loss / len(train_loader)
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_Y in val_loader:
                batch_X, batch_Y = batch_X.to(config['device']), batch_Y.to(config['device'])
                outputs = model(batch_X)
                loss = criterion(outputs, batch_Y)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        scheduler.step(avg_val_loss)
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        epoch_time = time.time() - epoch_start
        
        print(f"Epoch {epoch+1:3d}/{config['epochs']} | "
              f"Train Loss: {avg_train_loss:.6f} | "
              f"Val Loss: {avg_val_loss:.6f} | "
              f"Time: {epoch_time:.2f}s")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_tcn_lstm_model.pth')
            print(f"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ (æœ€ä½³ Val Loss: {best_val_loss:.6f})")
    
    print("="*60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print("="*60)
    
    # ç»“æœå¯è§†åŒ–
    if not config['test_mode']:
        plt.figure(figsize=(10, 5))
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Val Loss')
        plt.xlabel('Epochs')
        plt.ylabel('MSE Loss (Scaled)')
        plt.title('Training History')
        plt.legend()
        plt.grid(True)
        plt.show()
        
        visualize_predictions(model, val_loader, processor)

def visualize_predictions(model, loader, processor):
    model.eval()
    with torch.no_grad():
        for batch_X, batch_Y in loader:
            batch_X, batch_Y = batch_X.to(config['device']), batch_Y.to(config['device'])
            preds = model(batch_X)
            break
            
    preds_np = preds.cpu().numpy()
    targets_np = batch_Y.cpu().numpy()
    
    # åå½’ä¸€åŒ–
    preds_real = processor.scaler_Y.inverse_transform(preds_np)
    targets_real = processor.scaler_Y.inverse_transform(targets_np)
    
    plt.figure(figsize=(12, 8))
    for i in range(6):
        plt.subplot(3, 2, i+1)
        plt.plot(targets_real[:100, i], label='Ground Truth', alpha=0.7)
        plt.plot(preds_real[:100, i], label='Prediction', linestyle='--')
        plt.title(f'Feature: {processor.target_cols[i]}')
        plt.legend()
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    train_model()
