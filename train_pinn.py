import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import os
import time
import gc

# ==================== é…ç½®å‚æ•° ====================
config = {
    'data_path': 'enterprise_dataset/printer_enterprise_data.csv',  # åŸå§‹ CSV è·¯å¾„
    
    # â­ ä¿®æ”¹è¿™é‡Œï¼šå°†ç¼“å­˜ç›®å½•è®¾ç½®åœ¨ä½ çš„é«˜é€Ÿ SSD ä¸Š (ä¾‹å¦‚ Dç›˜ æˆ– Eç›˜)
    # è¿™å°†å¤§å¹…æå‡ DataLoader çš„è¯»å–é€Ÿåº¦
    'cache_dir': './data_cache/',  
    
    'seq_len': 200,
    'batch_size': 512,
    'hidden_dim': 128,
    'tcn_channels': [64, 64, 128],
    'lr': 1e-3,
    'epochs': 50,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'num_workers': 0, # ä½¿ç”¨ memmap æ—¶å»ºè®® 0
    'test_mode': False,
    'test_samples': 1000,
}

# ==================== 1. æœ€ç»ˆç‰ˆ æ•°æ®å¤„ç†å™¨ ====================

class EfficientDataProcessor:
    def __init__(self, data_path, seq_len, cache_dir, test_mode=False, test_samples=1000):
        self.data_path = data_path
        self.seq_len = seq_len
        self.cache_dir = cache_dir
        self.test_mode = test_mode
        self.test_samples = test_samples
        
        self.input_cols = ['ctrl_T_target', 'ctrl_speed_set', 'ctrl_heater_base']
        self.target_cols = ['temperature_C', 'vibration_disp_m', 'vibration_vel_m_s', 
                           'motor_current_A', 'pressure_bar', 'acoustic_signal']
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
        if os.path.exists(cache_dir) and os.listdir(cache_dir):
            print(f"ğŸ“¦ å‘ç°ç¼“å­˜ç›®å½•: {cache_dir}")
            self.load_metadata()
        else:
            print(f"ğŸ”„ ç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼€å§‹å¤„ç†æ•°æ®...")
            print(f"ğŸš€ ç¼“å­˜å°†å†™å…¥: {cache_dir}")
            os.makedirs(cache_dir, exist_ok=True)
            self.process_and_save()

    def process_and_save(self):
        """æµå¼å¤„ç†ï¼šè®¡ç®—ç»Ÿè®¡é‡ï¼Œå½’ä¸€åŒ–ï¼Œå¹¶å†™å…¥ mmap"""
        df = pd.read_csv(self.data_path)
        print(f"âœ… åŸå§‹æ•°æ®åŠ è½½: {df.shape}")
        
        # 1. ç¬¬ä¸€éæ‰«æï¼šè®¡ç®—å…¨å±€ç»Ÿè®¡é‡
        print("ğŸ“Š [Pass 1/2] è®¡ç®—å…¨å±€ç»Ÿè®¡é‡...")
        start_time = time.time()
        
        X_sum = np.zeros(len(self.input_cols), dtype=np.float64)
        X_sq_sum = np.zeros(len(self.input_cols), dtype=np.float64)
        Y_sum = np.zeros(len(self.target_cols), dtype=np.float64)
        Y_sq_sum = np.zeros(len(self.target_cols), dtype=np.float64)
        count = 0
        
        grouped = df.groupby('machine_id')
        
        for machine_id, group in grouped:
            group = group.sort_values('timestamp').reset_index(drop=True)
            X_raw = group[self.input_cols].values
            Y_raw = group[self.target_cols].values
            
            total_len = len(group)
            if total_len < self.seq_len + 1:
                continue
                
            n_windows = total_len - self.seq_len
            
            for i in range(n_windows):
                x_win = X_raw[i:i+self.seq_len]  # ç§»é™¤reshape(-1)ï¼Œä¿æŒäºŒç»´å½¢çŠ¶(seq_len, n_features)
                y_win = Y_raw[i+self.seq_len]
                
                # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ç»Ÿè®¡é‡ï¼Œè€Œä¸æ˜¯æ•´ä¸ªçª—å£çš„ç»Ÿè®¡é‡
                X_sum += x_win.mean(axis=0)  # å¯¹åºåˆ—ç»´åº¦æ±‚å¹³å‡ï¼Œä¿ç•™ç‰¹å¾ç»´åº¦
                X_sq_sum += (x_win**2).mean(axis=0)  # å¯¹åºåˆ—ç»´åº¦æ±‚å¹³å‡ï¼Œä¿ç•™ç‰¹å¾ç»´åº¦
                Y_sum += y_win
                Y_sq_sum += y_win**2
                count += 1
                
                if self.test_mode and count >= self.test_samples:
                    break
            
            if self.test_mode and count >= self.test_samples:
                break
                
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        self.mean_X = X_sum / count
        self.var_X = (X_sq_sum / count) - (self.mean_X ** 2)
        self.std_X = np.sqrt(self.var_X)
        
        self.mean_Y = Y_sum / count
        self.var_Y = (Y_sq_sum / count) - (self.mean_Y ** 2)
        self.std_Y = np.sqrt(self.var_Y)
        
        self.total_samples = count
        print(f"   æ ·æœ¬æ€»æ•°: {self.total_samples}")
        print(f"   è€—æ—¶: {time.time() - start_time:.2f}s")
        
        # åˆ’åˆ†æ•°æ®é›†
        self.split_idx = int(self.total_samples * 0.8)
        self.train_len = self.split_idx
        self.val_len = self.total_samples - self.split_idx
        
        print(f"   è®­ç»ƒé›†: {self.train_len}, éªŒè¯é›†: {self.val_len}")
        
        # â­ å…³é”®æ­¥éª¤ï¼šä¿å­˜ Scaler ç»Ÿè®¡é‡åˆ°ç£ç›˜
        print("ğŸ’¾ ä¿å­˜å½’ä¸€åŒ–å‚æ•°")
        scaler_path = os.path.join(self.cache_dir, 'scaler_stats.npz')
        np.savez(scaler_path,
                 mean_X=self.mean_X, std_X=self.std_X,
                 mean_Y=self.mean_Y, std_Y=self.std_Y)
        print(f"   å·²ä¿å­˜è‡³: {scaler_path}")

        # 2. ç¬¬äºŒéæ‰«æï¼šå½’ä¸€åŒ–å¹¶å†™å…¥ Memmap
        print("ğŸ’¾ [Pass 2/2] å†™å…¥ mmap ç¼“å­˜æ–‡ä»¶ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
        
        # å‡†å¤‡æ–‡ä»¶è·¯å¾„
        mmap_files = {
            'train_X': os.path.join(self.cache_dir, 'train_X.npy'),
            'train_Y': os.path.join(self.cache_dir, 'train_Y.npy'),
            'val_X': os.path.join(self.cache_dir, 'val_X.npy'),
            'val_Y': os.path.join(self.cache_dir, 'val_Y.npy'),
        }
        
        # åˆ›å»ºå¹¶åˆå§‹åŒ– Memmap æ–‡ä»¶ (w+ æ¨¡å¼ä¼šåˆ›å»ºå¹¶è¦†ç›–)
        self.train_X = np.lib.format.open_memmap(
            mmap_files['train_X'], dtype='float32', mode='w+', 
            shape=(self.train_len, self.seq_len, len(self.input_cols))
        )
        self.train_Y = np.lib.format.open_memmap(
            mmap_files['train_Y'], dtype='float32', mode='w+', 
            shape=(self.train_len, len(self.target_cols))
        )
        self.val_X = np.lib.format.open_memmap(
            mmap_files['val_X'], dtype='float32', mode='w+', 
            shape=(self.val_len, self.seq_len, len(self.input_cols))
        )
        self.val_Y = np.lib.format.open_memmap(
            mmap_files['val_Y'], dtype='float32', mode='w+', 
            shape=(self.val_len, len(self.target_cols))
        )
        
        # å†™å…¥æŒ‡é’ˆ
        train_ptr = 0
        val_ptr = 0
        current_idx = 0
        
        for machine_id, group in grouped:
            group = group.sort_values('timestamp').reset_index(drop=True)
            X_raw = group[self.input_cols].values
            Y_raw = group[self.target_cols].values
            
            total_len = len(group)
            if total_len < self.seq_len + 1:
                continue
                
            n_windows = total_len - self.seq_len
            
            for i in range(n_windows):
                if self.test_mode and current_idx >= self.test_samples:
                    break
                
                # å½’ä¸€åŒ– (X)
                x_win = X_raw[i:i+self.seq_len]
                x_norm = (x_win - self.mean_X) / self.std_X
                
                # å½’ä¸€åŒ– (Y)
                y_win = Y_raw[i+self.seq_len]
                y_norm = (y_win - self.mean_Y) / self.std_Y
                
                # å†™å…¥ Memmap
                if current_idx < self.train_len:
                    self.train_X[train_ptr] = x_norm.astype(np.float32)
                    self.train_Y[train_ptr] = y_norm.astype(np.float32)
                    train_ptr += 1
                else:
                    self.val_X[val_ptr] = x_norm.astype(np.float32)
                    self.val_Y[val_ptr] = y_norm.astype(np.float32)
                    val_ptr += 1
                    
                current_idx += 1
            
            if self.test_mode and current_idx >= self.test_samples:
                break
        
        print("âœ… ç¼“å­˜å†™å…¥å®Œæˆï¼")
        
        # ä¸åˆ é™¤mmapå¯¹è±¡ï¼Œè€Œæ˜¯å…³é—­å¹¶é‡æ–°ä»¥åªè¯»æ¨¡å¼åŠ è½½
        # ä¸ºäº†ç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥è®¿é—®è¿™äº›å±æ€§ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°åŠ è½½å®ƒä»¬
        # åˆ é™¤è¿™äº›ä¸´æ—¶å±æ€§ï¼Œç„¶ååœ¨load_metadataä¸­é‡æ–°åŠ è½½
        del self.train_X, self.train_Y, self.val_X, self.val_Y
        gc.collect()
        
        # é‡æ–°åŠ è½½æ•°æ®ä»¥ä¾›è®­ç»ƒä½¿ç”¨
        self.load_metadata()

    def load_metadata(self):
        """åŠ è½½å·²ç¼“å­˜çš„ Memmap å’Œ Scaler"""
        # 1. åŠ è½½ Scaler å‚æ•°
        scaler_path = os.path.join(self.cache_dir, 'scaler_stats.npz')
        if not os.path.exists(scaler_path):
             raise FileNotFoundError(f"æ‰¾ä¸åˆ° Scaler æ–‡ä»¶: {scaler_path}ï¼Œè¯·é‡æ–°ç”Ÿæˆç¼“å­˜ã€‚")
        
        data = np.load(scaler_path)
        self.mean_X = data['mean_X']
        self.std_X = data['std_X']
        self.mean_Y = data['mean_Y']
        self.std_Y = data['std_Y']
        print("âœ… å½’ä¸€åŒ–å‚æ•°åŠ è½½æˆåŠŸ")
        
        # 2. åŠ è½½ Memmap æ•°æ®
        self.train_X = np.load(os.path.join(self.cache_dir, 'train_X.npy'), mmap_mode='r')
        self.train_Y = np.load(os.path.join(self.cache_dir, 'train_Y.npy'), mmap_mode='r')
        self.val_X = np.load(os.path.join(self.cache_dir, 'val_X.npy'), mmap_mode='r')
        self.val_Y = np.load(os.path.join(self.cache_dir, 'val_Y.npy'), mmap_mode='r')
        
        self.train_len = self.train_X.shape[0]
        self.val_len = self.val_X.shape[0]
        self.total_samples = self.train_len + self.val_len
        print(f"âœ… æ•°æ®æ˜ å°„åŠ è½½æˆåŠŸ: Train {self.train_len}, Val {self.val_len}")

    def inverse_transform_y(self, y_norm):
        """å°†å½’ä¸€åŒ–çš„é¢„æµ‹å€¼è¿˜åŸä¸ºçœŸå®ç‰©ç†å€¼ (ç”¨äºå¯è§†åŒ–)"""
        return y_norm * self.std_Y + self.mean_Y

class MMapDataset(Dataset):
    def __init__(self, X_mmap, Y_mmap):
        self.X = X_mmap
        self.Y = Y_mmap
        
    def __len__(self):
        return self.X.shape[0]
    
    def __getitem__(self, idx):
        return self.X[idx], self.Y[idx]

# ==================== 2. æ¨¡å‹å®šä¹‰ ====================

class TemporalBlock(nn.Module):
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super(TemporalBlock, self).__init__()
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,
                              stride=stride, padding=padding, dilation=dilation)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        self.net = nn.Sequential(self.conv1, self.relu1, self.dropout1)
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        self.kernel_size = kernel_size
        self.dilation = dilation
        
    def forward(self, x):
        out = self.net(x)
        pad = (self.kernel_size - 1) * self.dilation
        out = out[:, :, :-pad]
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)

class TCN(nn.Module):
    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):
        super(TCN, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            padding = (kernel_size - 1) * dilation_size
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,
                                   dilation=dilation_size, padding=padding, dropout=dropout)]
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        x = x.transpose(1, 2)
        out = self.network(x)
        return out.transpose(1, 2)

class TCNLSTMModel(nn.Module):
    def __init__(self, input_dim, tcn_channels, hidden_dim, output_dim):
        super(TCNLSTMModel, self).__init__()
        self.tcn = TCN(input_dim, tcn_channels)
        tcn_output_dim = tcn_channels[-1]
        self.lstm = nn.LSTM(tcn_output_dim, hidden_dim, num_layers=2, 
                           batch_first=True, dropout=0.1, bidirectional=False)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x):
        tcn_out = self.tcn(x)
        lstm_out, (h_n, c_n) = self.lstm(tcn_out) 
        last_step_out = lstm_out[:, -1, :]
        prediction = self.fc(last_step_out)
        return prediction

# ==================== 3. è®­ç»ƒä¸å¯è§†åŒ– ====================

def visualize_predictions(model, loader, processor):
    """ä½¿ç”¨çœŸå®çš„ç‰©ç†å•ä½è¿›è¡Œå¯è§†åŒ–"""
    model.eval()
    with torch.no_grad():
        for batch_X, batch_Y in loader:
            batch_X, batch_Y = batch_X.to(config['device']), batch_Y.to(config['device'])
            preds = model(batch_X)
            break
            
    # ç§»å› CPU å¹¶è½¬ä¸º numpy
    preds_np = preds.cpu().numpy()
    targets_np = batch_Y.cpu().numpy()
    
    # â­ ä½¿ç”¨ processor çš„æ–¹æ³•è¿›è¡Œåå½’ä¸€åŒ–
    preds_real = processor.inverse_transform_y(preds_np)
    targets_real = processor.inverse_transform_y(targets_np)
    
    plt.figure(figsize=(12, 8))
    for i in range(6):
        plt.subplot(3, 2, i+1)
        # åªç”»å‰100ä¸ªç‚¹
        plt.plot(targets_real[:100, i], label='Ground Truth', alpha=0.7)
        plt.plot(preds_real[:100, i], label='Prediction', linestyle='--')
        plt.title(f'Feature: {processor.target_cols[i]}')
        plt.legend()
        plt.grid(True)
    plt.tight_layout()
    plt.show()

def train_model():
    print("="*60)
    print("ğŸš€ æœ€ç»ˆç‰ˆ TCN-LSTM è®­ç»ƒ (æ”¯æŒ Scaler å­˜å–)")
    print("="*60)
    
    processor = EfficientDataProcessor(
        config['data_path'], 
        config['seq_len'],
        config['cache_dir'], # è¿™é‡Œä½¿ç”¨ä½ åœ¨ config ä¸­è®¾ç½®çš„é«˜é€Ÿç£ç›˜è·¯å¾„
        test_mode=config['test_mode'],
        test_samples=config['test_samples']
    )
    
    train_dataset = MMapDataset(processor.train_X, processor.train_Y)
    val_dataset = MMapDataset(processor.val_X, processor.val_Y)
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], 
                             shuffle=True, num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], 
                           shuffle=False, num_workers=0, pin_memory=True)
    
    # åˆå§‹åŒ–æ¨¡å‹
    input_dim = len(processor.input_cols)
    output_dim = len(processor.target_cols)
    
    model = TCNLSTMModel(input_dim, config['tcn_channels'], config['hidden_dim'], output_dim)
    
    if torch.cuda.device_count() > 1:
        print(f"ğŸ® ä½¿ç”¨ {torch.cuda.device_count()} ä¸ª GPU!")
        model = nn.DataParallel(model)
        
    model = model.to(config['device'])
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    criterion = nn.MSELoss()
    
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    best_val_loss = float('inf')
    
    for epoch in range(config['epochs']):
        epoch_start = time.time()
        model.train()
        epoch_loss = 0
        
        for batch_idx, (batch_X, batch_Y) in enumerate(train_loader):
            batch_X, batch_Y = batch_X.to(config['device']), batch_Y.to(config['device'])
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_Y)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            
        avg_train_loss = epoch_loss / len(train_loader)
        
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_Y in val_loader:
                batch_X, batch_Y = batch_X.to(config['device']), batch_Y.to(config['device'])
                outputs = model(batch_X)
                loss = criterion(outputs, batch_Y)
                val_loss += loss.item()
                
        avg_val_loss = val_loss / len(val_loader)
        scheduler.step(avg_val_loss)
        
        epoch_time = time.time() - epoch_start
        print(f"Epoch {epoch+1:3d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | Time: {epoch_time:.2f}s")
        
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_tcn_lstm_model.pth')
            print(f"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜")

    # è®­ç»ƒç»“æŸåå¯è§†åŒ–
    if not config['test_mode']:
        print("\nç”Ÿæˆé¢„æµ‹å¯è§†åŒ–å›¾è¡¨...")
        visualize_predictions(model, val_loader, processor)

if __name__ == "__main__":
    import matplotlib.pyplot as plt
    train_model()
